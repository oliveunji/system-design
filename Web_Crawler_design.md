# 웹 크롤러 설계

## 웹 크롤러란?
robot, spider로 불리며 새로 올라온 갱신된 컨텐츠를 찾아내는 일

이용사례
- 검색엔진 인덱싱: 웹 페이지를 모아 local Index를 만든다
- 웹 아카이빙: 장기 보관용 정보 모으는 절차
- 웹 마이닝: 인터넷의 유용한 지식 도출
- 웹 모니터링: 저작권이나 상표권 침해되는 사례 모니터링

## 1. 문제 이해 및 범위 설정
1. 크롤러의 주된용도? 인덱스 생성용인지, 데이터 마이닝용인지?
    - 검색엔진 인덱싱용
2. 매일 얼마나 많은 페이지를 수집하는지?
    - 10억개(1 billion)
3. 새로 만들어지거나 수정된 웹 페이지도 고려사항인지?
    - 고려 대상임
4. 수집한 웹 페이지는 얼마나 저장해야 하는지?
    - 5년간
5. 중복된 컨텐츠는 어떻게 처리하는지?
    - 무시함

### 크롤링 기본 알고리즘
1. URL 리스트 주어지면 모든 웹 페이지 다운로드
2. 다운받은 웹페이지에서 URL 추출
3. 추출된 URL을 목록에 추가한 후 위의 과정 반복

### 좋은 웹 크롤러가 만족해야 하는 상황
1. 규모확장성: 웹은 크고 방대하기 때문에 Parallelism 잘 활용해야함
2. 안정성: 웹은 함정으로 가득하기 때문에 악성코드나, 링크를 주의해야 한다
3. 예절: 수집 대상 사이트에 짧은 시간동안 너무 많은 요청을 보내면 안된다
4. 확장성: 새로운 형태의 컨텐츠 지원이 쉬워야 한다.

### 규모산정
- 매달 10억개 다운로드
- QPS = 1,000,000,000 / 30 / 24 / 3600 => 400 p/s 초당 400개 정도
- Peak QPS를 800개로 가정
- 페이지 평균크기를 500K로 가정
- 10억 페이지 * 500K = 500TB / 월
- 5년간 저장시 500TB * 12 * 5 = 30 PB

## 2. 계략적인 설계안 제시 및 동의 구하기 
<img width="597" alt="image" src="https://user-images.githubusercontent.com/39396725/225779857-e1ddad3d-342e-436a-9c49-d8c81771e4e3.png">

## 3. 상세 설계
1. DFS vs BFS
    - 크롤러는 보통 BFS를 사용한다. 이경우 같은 페이지에서 참고하는 하위 페이지 크롤링 하며 polite 하지 않은 크롤러가 될 수 있기 때문에 주의해야 한다
    - 일반 BFS에는 URL 간에 우선순위를 두지 않으나, 페이지 순위나 트래픽 양, 업데이트 빈도에 따라 우선순위를 비교해야 한다. 
2. 미수집 URL 저장소
    - 동일한 URL에 대해 시차를 두고 요청하기 위해, 각 다운로드 Thread는 별도의 FIFO 큐를 두고 개발한다. 
3. HTML 다운로더
    - robots.txt에 규칙이 정의되어 있다. 
4. 안정성 확보전략
    - 안정해시 설계한다
    - 크롤링 상태 수집 및 데이터를 저장한다
    - 예외처리및 데이터를 검증한다
5. 문제있는 컨텐츠 감지 및 회피
    - 중복컨텐츠의 경우 Hash나, Checksum을 이용한다
    - Spider Trap이라 불리우는 무한히 깊은 디렉터리 구조 가진 경우, URL 최대 길이 제한등을 이용한다
    - 가치없는 컨텐츠 (광고, 스팸 등등)의 경우는 필터링한다.  

# 4. 추가 논의사항
- Server Side Rendering
- 페이지 필터링
- DB 다중화 및 샤딩
- 수평적 규모확장
- 가용성, 일관성, 안정성
- Analytics
